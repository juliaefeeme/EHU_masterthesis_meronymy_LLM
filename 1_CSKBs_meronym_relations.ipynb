{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#<p><center> **COMMONSENSE KNOWLEDGE BASES: MERONYMIC RELATIONSHIPS ACQUISITION AND THEIR VERIFICATION WITH LARGE LANGUAGE MODELS**\n",
        "\n",
        "---\n",
        "\n",
        "<p><center> Master thesis - MS in Language Analysis and Proccessing\n",
        "<p><center> Julia Fidalgo Mari√±o\n",
        "<p><center> Supervised by German Rigau</center></p>  \n",
        "     "
      ],
      "metadata": {
        "id": "aRjnA5V5g-hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "#<p><center> **COMMONSENSE KNOWLEDGE BASES (CSKB)**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7M6Ch0HEuV31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KVOlRCdiyBn",
        "outputId": "a3dc0034-a2d9-42bb-f100-a97683e8221e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3KPH1Fyg7uB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pandas\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSj2GZz2i5rg",
        "outputId": "abe10f29-ff28-4125-82e2-14521f8ca9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. FILES AND FORMAT**\n",
        "\n",
        "*   hasPartKB\n",
        "*   VisualGenome\n",
        "*   TransOMCS\n",
        "*   Ascent++\n",
        "*   Quasimodo\n",
        "*   Uncommonsense\n",
        "*   AristoTupleKB\n",
        "*   ConceptNet\n",
        "\n",
        "These CSKB need to be processed in order to get this format\n",
        "\n",
        "```\n",
        "Word1 Word2 Offset1 Offset2\n",
        "```"
      ],
      "metadata": {
        "id": "krvfF_s9js04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each CSKB file has to be exploited differently. The objective is to keep only those relationships with the labels *PartOf*, *HasA* or *MemberOf* or similar."
      ],
      "metadata": {
        "id": "lU4u52Ctsv_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.1. HasPartKB**"
      ],
      "metadata": {
        "id": "Pu9sFR7Pto_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_haspart(input_file):\n",
        "  output_file=\"drive/My Drive/Colab/TFM/haspart-ord.csv\"\n",
        "  df = pd.read_csv(input_file, delimiter=\" \")\n",
        "\n",
        "  df.columns=['Relation', 'Word1', 'Word2', 'Score', 'Synset1', 'Offset1', 'Synset2', 'Offset2'] #rename columns\n",
        "  #print(df.head(4))\n",
        "  df.insert(loc=0, column='Resource', value=\"HP\") #add column\n",
        "  df_ord = df.reindex(columns=['Word1', 'Word2', 'Offset1', 'Offset2', 'Resource']) #reorder columns\n",
        "  df_ord.to_csv(output_file, index=False) #save as csv\n",
        "  print(df_ord)\n",
        "\n",
        "input_file=\"drive/My Drive/Colab/TFM/adimen-haspart.txt\"\n",
        "process_haspart(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK_V2QgAnH0w",
        "outputId": "fbf28501-2015-43f8-a9e9-d54799bcbc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Word1         Word2     Offset1     Offset2 Resource\n",
            "0               animal         belly  00015388-n  05556943-n       HP\n",
            "1               anuran         belly  01639765-n  05556943-n       HP\n",
            "2                aphid       abdomen  02252226-n  05556943-n       HP\n",
            "3             arachnid       abdomen  01769347-n  05556943-n       HP\n",
            "4            arthropod       abdomen  01767661-n  05556943-n       HP\n",
            "...                ...           ...         ...         ...      ...\n",
            "34888           animal        zygote  00015388-n  05431926-n       HP\n",
            "34889              egg        zygote  01460457-n  05431926-n       HP\n",
            "34890    female_'s_egg        zygote  01460457-n  05431926-n       HP\n",
            "34891           specie        zygote  08110373-n  05431926-n       HP\n",
            "34892  large_intestine  fermentation  05535484-n  13575433-n       HP\n",
            "\n",
            "[34893 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.2. VisualGenome**"
      ],
      "metadata": {
        "id": "TfbSXoqVv4Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_visualgen(input_file):\n",
        "  output_file=\"drive/My Drive/Colab/TFM/visualg-ord.csv\"\n",
        "\n",
        "  if input_file.endswith('.csv'):\n",
        "    df = pd.read_csv(input_file)\n",
        "  elif input_file.endswith('.txt'):\n",
        "    df = pd.read_csv(input_file, delimiter=\" \")\n",
        "  else:\n",
        "    print(f\"File {input_file} is not compatible\")\n",
        "  #print(len(df))\n",
        "\n",
        "  df.columns =['Relation', 'Offset-rel', 'Synset1', 'Offset1', 'Synset2', 'Offset2', 'Times']\n",
        "  df = df.drop('Offset-rel', axis = 1)\n",
        "  df2=df[(df['Relation']=='have.v.01') | (df['Relation']=='have.v.02')] #labels\n",
        "  df_ord = df2.reindex(columns=['Offset1','Offset2'])\n",
        "  df_ord.insert(loc=0,column='Word1',value=None) #add columns\n",
        "  df_ord.insert(loc=1,column='Word2',value=None)\n",
        "  df_ord.insert(loc=4,column='Resource',value=\"VG\")\n",
        "\n",
        "  print(df_ord)\n",
        "  df_ord.to_csv(output_file, index = False)\n",
        "\n",
        "input_file = \"drive/My Drive/Colab/TFM/visualgenometriplets.txt\"\n",
        "process_visualgen(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYOfoV2cv395",
        "outputId": "264a37e9-808c-47d0-c826-10a030acb73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Word1 Word2     Offset1     Offset2 Resource\n",
            "149613  None  None  02667576-n  03017428-n       VG\n",
            "149614  None  None  02667576-n  03046257-n       VG\n",
            "149615  None  None  02667576-n  04587648-n       VG\n",
            "149616  None  None  05556943-n  02784732-n       VG\n",
            "149617  None  None  05556943-n  00928077-n       VG\n",
            "...      ...   ...         ...         ...      ...\n",
            "180971  None  None  04468005-n  06807198-n       VG\n",
            "180972  None  None  04522168-n  02430045-n       VG\n",
            "180973  None  None  04522168-n  01621127-n       VG\n",
            "180974  None  None  04587648-n  07944408-n       VG\n",
            "180975  None  None  10787470-n  06793426-n       VG\n",
            "\n",
            "[31363 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.3. TransOMCS**"
      ],
      "metadata": {
        "id": "4U5LVWUBwpoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_trans(input_file):\n",
        "  output_file=\"drive/My Drive/Colab/TFM/transomcs-ord.csv\"\n",
        "\n",
        "  if input_file.endswith('.csv'):\n",
        "    df = pd.read_csv(input_file)\n",
        "  elif input_file.endswith('.txt'):\n",
        "    df = pd.read_csv(input_file, delimiter=\"\\t\")\n",
        "  else:\n",
        "    print(f\"File {input_file} is not compatible\")\n",
        "\n",
        "  df.columns =['Word2', 'Relation', 'Word1', 'Score']\n",
        "  #print(df.head(4))\n",
        "  df.insert(loc=0, column='Offset1', value=None)\n",
        "  df.insert(loc=1, column='Offset2', value=None)\n",
        "  df.insert(loc=2,column='Resource',value=\"T\")\n",
        "\n",
        "  df_rel=df[(df['Relation']=='PartOf')|(df['Relation']=='HasA')]\n",
        "  df_ord = df_rel.reindex(columns=['Word1','Word2','Offset1','Offset2', 'Resource']).astype(str)\n",
        "  print(df_ord)\n",
        "  df_ord.to_csv(output_file, index = False)\n",
        "\n",
        "trans_file = \"drive/My Drive/Colab/TFM/TransOMCS_full.txt\"\n",
        "process_trans(trans_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7OsuLzBw0rw",
        "outputId": "f16e8fe4-9d1d-4568-ef18-dda529f0993f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Word1           Word2 Offset1 Offset2 Resource\n",
            "3771        right          people    None    None        T\n",
            "3772      feeling          animal    None    None        T\n",
            "3773      opinion        everyone    None    None        T\n",
            "3774      bedroom           house    None    None        T\n",
            "3775         name      everything    None    None        T\n",
            "...           ...             ...     ...     ...      ...\n",
            "18350511        i  osteoarthritis    None    None        T\n",
            "18354221   belton          belton    None    None        T\n",
            "18398143   nuance          nuance    None    None        T\n",
            "18435327   fealty          fealty    None    None        T\n",
            "18481527        i           missy    None    None        T\n",
            "\n",
            "[1920447 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4. Ascent++**"
      ],
      "metadata": {
        "id": "kjUGi9tIxhHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_ascent(input_file):\n",
        "  df=pd.read_csv(asc_file, sep=',')\n",
        "  #print(len(df))\n",
        "  #print(df['relation'].unique().tolist())\n",
        "\n",
        "  df.insert(loc=0, column='Offset1', value=None)\n",
        "  df.insert(loc=1, column='Offset2', value=None)\n",
        "  df.insert(loc=2, column='Resource', value=\"A++\")\n",
        "  df_part=df[(df['relation']=='PartOf')|(df['relation']=='MadeOf')|(df['relation']=='HasA')]\n",
        "  df_part = df_part.drop(['subject_type','head','tail','subject','predicate','saliency','typicality','facets'], axis = 1) #delete columns\n",
        "  df_part2 = df_part.rename(columns={'primary_subject':'Word1','object':'Word2'}) #change name of the original columns\n",
        "  df_ord = df_part2.reindex(columns=['Word1','Word2','Offset1','Offset2', 'Resource'])\n",
        "  print(df_ord)\n",
        "\n",
        "  df_ord.to_csv(\"drive/My Drive/Colab/TFM/ascent-ord.csv\", index = False)\n",
        "\n",
        "asc_file = \"drive/My Drive/Colab/TFM/ascentpp.csv\"\n",
        "process_ascent(asc_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrYE2ai_xvjB",
        "outputId": "fe4673eb-4782-463d-db9a-7bf93e6129e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Word1                  Word2 Offset1 Offset2 Resource\n",
            "25       aardvark   elaborate structures    None    None      A++\n",
            "30       aardvark             thick skin    None    None      A++\n",
            "60            aba              liability    None    None      A++\n",
            "61            aba               failures    None    None      A++\n",
            "93            aba        410,000 members    None    None      A++\n",
            "...           ...                    ...     ...     ...      ...\n",
            "2054457      year               24 hours    None    None      A++\n",
            "2054568      year                meeting    None    None      A++\n",
            "2054618      year           legal status    None    None      A++\n",
            "2054750    yogurt       limited lifetime    None    None      A++\n",
            "2054880  zucchini  the most antioxidants    None    None      A++\n",
            "\n",
            "[106714 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.5. Quasimodo**"
      ],
      "metadata": {
        "id": "CYEHzLVBxwSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_quasimodo(input_file):\n",
        "  output_file=\"drive/My Drive/Colab/TFM/quasimodo-ord.csv\"\n",
        "\n",
        "  if input_file.endswith('.csv'):\n",
        "    df = pd.read_csv(input_file)\n",
        "  elif input_file.endswith('.txt'):\n",
        "    df = pd.read_csv(input_file, delimiter=\" \")\n",
        "  elif input_file.endswith('.tsv'):\n",
        "    df = pd.read_csv(input_file, delimiter=\"\\t\")\n",
        "  else:\n",
        "    print(f\"File {input_file} is not compatible\")\n",
        "\n",
        "  df.insert(loc=0, column='Offset1', value=None)\n",
        "  df.insert(loc=1, column='Offset2', value=None)\n",
        "  df.insert(loc=2, column='Resource', value=\"Q\")\n",
        "  df_part=df[(df['predicate']=='have')|(df['predicate']=='has_body_part')]\n",
        "  df_part = df_part.drop(['modality','is_negative','score','typicality','saliency'], axis = 1)\n",
        "  df_part2 = df_part.rename(columns={'subject':'Word1','object':'Word2'})\n",
        "  df_ord = df_part2.reindex(columns=['Word1','Word2','Offset1','Offset2','Resource']).astype(str)\n",
        "  print(df_ord)\n",
        "\n",
        "  df_ord.to_csv(output_file, index = False)\n",
        "\n",
        "qua_file = \"drive/My Drive/Colab/TFM/quasimodo.tsv\"\n",
        "process_quasimodo(qua_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA0o1dyBxzZi",
        "outputId": "c827a3d7-6ec2-4dfe-bec3-abafacff5bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        Word1             Word2 Offset1 Offset2 Resource\n",
            "1                         man            nipple    None    None        Q\n",
            "7                        fish             blood    None    None        Q\n",
            "15                       baby              hair    None    None        Q\n",
            "27             red blood cell           nucleus    None    None        Q\n",
            "28                    toddler              hair    None    None        Q\n",
            "...                       ...               ...     ...     ...      ...\n",
            "6274216  two blue eyed parent  brown eyed child    None    None        Q\n",
            "6274231   leopard geckos tail              tail    None    None        Q\n",
            "6274293           banana skin             skins    None    None        Q\n",
            "6274299            hip thrust           muscles    None    None        Q\n",
            "6274499                 woman          clothing    None    None        Q\n",
            "\n",
            "[145133 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.6. Uncommonsense**"
      ],
      "metadata": {
        "id": "MXBDjZ_Rxz3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_uncommon(input_file):\n",
        "\n",
        "  if input_file.endswith('.csv'):\n",
        "    df = pd.read_csv(input_file)\n",
        "  elif input_file.endswith('.txt'):\n",
        "    df = pd.read_csv(input_file, delimiter=\" \")\n",
        "  elif input_file.endswith('.tsv'):\n",
        "    df = pd.read_csv(input_file, delimiter=\"\\t\")\n",
        "  else:\n",
        "    print(f\"File {input_file} is not compatible\")\n",
        "\n",
        "  df.insert(loc=0, column='Offset1', value=None)\n",
        "  df.insert(loc=1, column='Offset2', value=None)\n",
        "  df.insert(loc=2, column='Resource', value=\"Un\")\n",
        "  df_part=df[(df['relation']=='HasA')]\n",
        "  #print(df_part.head(10))\n",
        "  df_part2 = df_part.rename(columns={'head':'Word1','tail':'Word2'})\n",
        "  df_ord = df_part2.reindex(columns=['Word1','Word2','Offset1','Offset2','Resource']).astype(str)\n",
        "  print(df_ord)\n",
        "\n",
        "  df_ord.to_csv(\"drive/My Drive/Colab/TFM/uncommon-ord.csv\", index = False)\n",
        "\n",
        "un_file = \"drive/My Drive/Colab/TFM/uncommon-neg.tsv\"\n",
        "process_uncommon(un_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvTdtLI8x9qu",
        "outputId": "5fdb3b3d-84f9-46a2-f1a0-44b6a9aa608c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Word1                   Word2 Offset1 Offset2 Resource\n",
            "22        disability               institute    None    None       Un\n",
            "170      grandfather  the graves of soldiers    None    None       Un\n",
            "429          hearing    significant activity    None    None       Un\n",
            "443           osprey               the range    None    None       Un\n",
            "524      oil company     omega-9 fatty acids    None    None       Un\n",
            "...              ...                     ...     ...     ...      ...\n",
            "7999846      cruiser              long wicks    None    None       Un\n",
            "7999857      country     long history of use    None    None       Un\n",
            "7999941       cornea               much spin    None    None       Un\n",
            "7999976    polyester      a half-load option    None    None       Un\n",
            "7999983       bidder            clear appeal    None    None       Un\n",
            "\n",
            "[117552 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.7. AristoTupleKB**"
      ],
      "metadata": {
        "id": "EcnFNrctx_iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ar_file = \"drive/My Drive/Colab/TFM/AristoCombined.tsv\"\n",
        "df6 = pd.read_csv(ar_file, delimiter='\\t')\n",
        "#print(df6.head(10))\n",
        "df6.insert(loc=0, column='Offset1', value=None)\n",
        "df6.insert(loc=1, column='Offset2', value=None)\n",
        "df6.insert(loc=2, column='Resource', value=\"Ar\")\n",
        "df6_part=df6[(df6['Pred']=='has-part')] #selection 1\n",
        "df6_part2 = df6_part.rename(columns={'Arg1':'Word1','Arg2':'Word2'})\n",
        "df6_ord = df6_part2.reindex(columns=['Word1','Word2','Offset1','Offset2','Resource']).astype(str)\n",
        "#print(df6_ord)\n",
        "\n",
        "df6_ord.to_csv(\"drive/My Drive/Colab/TFM/Aristo-ord2.csv\", index = False)"
      ],
      "metadata": {
        "id": "oxzkgFwmyDmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar_file = \"drive/My Drive/Colab/TFM/AristoCombined.tsv\"\n",
        "df6 = pd.read_csv(ar_file, delimiter='\\t')\n",
        "#print(df6.head(10))\n",
        "df6.insert(loc=0, column='Offset1', value=None)\n",
        "df6.insert(loc=1, column='Offset2', value=None)\n",
        "df6.insert(loc=2, column='Resource', value=\"Ar\")\n",
        "df6_part2=df6[(df6['Pred']=='is-part-of')] #selection 2\n",
        "df6_part2 = df6_part2.rename(columns={'Arg1':'Word2','Arg2':'Word1'}) #different order\n",
        "df6_ord2 = df6_part2.reindex(columns=['Word1','Word2','Offset1','Offset2','Resource']).astype(str)\n",
        "#print(df6_ord2)\n",
        "\n",
        "df6_ord2.to_csv(\"drive/My Drive/Colab/TFM/Aristo-ord3.csv\", index = False)\n",
        "df61 = pd.read_csv(\"drive/My Drive/Colab/TFM/Aristo-ord2.csv\")\n",
        "df62 = pd.read_csv(\"drive/My Drive/Colab/TFM/Aristo-ord3.csv\")"
      ],
      "metadata": {
        "id": "FbPPO9LFmDCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined = pd.concat([df61, df62], ignore_index=True).astype(str)\n",
        "combined.to_csv(\"drive/My Drive/Colab/TFM/aristo-ord.csv\", index=False)\n",
        "print(len(combined))\n",
        "#print(combined.head(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmsUc-I1mGsX",
        "outputId": "b8c89972-64ae-4b1e-aa4a-1d032fbdaffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df61))\n",
        "print(len(df62))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrBI5U3fwEcM",
        "outputId": "f6455884-dd93-48bd-efaa-0ecc90be3a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32587\n",
            "7438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.8. ConceptNet**"
      ],
      "metadata": {
        "id": "HnM0M0OryEBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*1.8.1. API*"
      ],
      "metadata": {
        "id": "Ty7-h6CbyOJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def query_conceptnet(offset):\n",
        "    base_url = \"http://api.conceptnet.io\"\n",
        "    endpoint = \"/query\"\n",
        "\n",
        "    query = {\n",
        "        \"limit\": 1000,\n",
        "        \"offset\": offset,\n",
        "        \"language\": \"en\",\n",
        "        \"start\": \"/c/en\",\n",
        "        \"end\": \"/c/en\",\n",
        "        \"rel\": [\"/r/PartOf\", \"/r/MadeOf\"]\n",
        "    }\n",
        "\n",
        "    response = requests.get(f\"{base_url}{endpoint}\", params=query)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        edges = data[\"edges\"]\n",
        "        relation_pairs = set()\n",
        "        for edge in edges:\n",
        "            start_label = edge[\"start\"][\"label\"]\n",
        "            end_label = edge[\"end\"][\"label\"]\n",
        "            relation_pairs.add((start_label, end_label))\n",
        "        return relation_pairs\n",
        "    else:\n",
        "        return set()\n",
        "\n",
        "def get_english_relations():\n",
        "    with Pool(processes=4) as pool:\n",
        "        offsets = range(0, 10000, 100)\n",
        "        results = pool.map(query_conceptnet, offsets)\n",
        "    relation_pairs = set().union(*results)\n",
        "    return relation_pairs\n",
        "\n",
        "def save_to_csv(relation_pairs, filename):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Word1\", \"Word2\"])\n",
        "        for pair in relation_pairs:\n",
        "            writer.writerow(pair)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    relation_pairs = get_english_relations()\n",
        "    csv_filename = \"drive/My Drive/Colab/TFM/conceptnet-rel.csv\"\n",
        "    save_to_csv(relation_pairs, csv_filename)\n",
        "    print(\"ok\")"
      ],
      "metadata": {
        "id": "uowxzWyqsjVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef77387-4b92-439e-9be8-373d24a8f90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*1.8.2. File and format*"
      ],
      "metadata": {
        "id": "vb4PD6L8yWT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_conceptnet(input_file):\n",
        "  output_file=\"drive/My Drive/Colab/TFM/conceptnet-ord.csv\"\n",
        "\n",
        "  df = pd.read_csv(input_file, delimiter=',')\n",
        "  df.insert(loc=0, column='Offset1', value=None)\n",
        "  df.insert(loc=1, column='Offset2', value=None)\n",
        "  df.insert(loc=2, column='Resource', value=\"C\")\n",
        "  df_ord = df.reindex(columns=['Word1','Word2','Offset1','Offset2','Resource']).astype(str)\n",
        "  print(df_ord)\n",
        "\n",
        "  df_ord.to_csv(output_file, index = False)\n",
        "\n",
        "input_file = \"drive/My Drive/Colab/TFM/conceptnet-rel.csv\"\n",
        "process_conceptnet(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdjpYNVHyYiz",
        "outputId": "4e250557-9980-41ce-baed-897caf1052b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Word1                 Word2 Offset1 Offset2 Resource\n",
            "0        inflammation  inflammatory disease    None    None        C\n",
            "1            ≈°iauliai             samogitia    None    None        C\n",
            "2                byte                  word    None    None        C\n",
            "3     corpus callosum             forebrain    None    None        C\n",
            "4     Hottentot bread       elephant's foot    None    None        C\n",
            "...               ...                   ...     ...     ...      ...\n",
            "9131          Bishkek            Kyrgyzstan    None    None        C\n",
            "9132            is√®re           rh√¥ne alpes    None    None        C\n",
            "9133    Libyan dirham          Libyan dinar    None    None        C\n",
            "9134        east asia                taipei    None    None        C\n",
            "9135         syllable                  word    None    None        C\n",
            "\n",
            "[9136 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2. WORDNET**\n",
        "\n",
        "For those CSKB that do not have their four columns completed, it is needed the access to WordNet in order to obtain the offsets or synsets for every element the file."
      ],
      "metadata": {
        "id": "1hLq10fM7qjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "3Rf93NcUBI0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.1. Get offset from a word**"
      ],
      "metadata": {
        "id": "oFah7III8G3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synset_offset(word):\n",
        "    word=str(word) #(AttributeError: 'float' object has no attribute 'lower')\n",
        "    synsets=wn.synsets(word)\n",
        "    if synsets:\n",
        "        offset=synsets[0].offset()\n",
        "        return f\"{offset:08d}-{synsets[0].pos()}\"\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "wDIc-8dL79Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['drive/My Drive/Colab/TFM/transomcs-ord.csv',\n",
        "        'drive/My Drive/Colab/TFM/ascent-ord.csv',\n",
        "         'drive/My Drive/Colab/TFM/quasimodo-ord.csv',\n",
        "         'drive/My Drive/Colab/TFM/uncommon-ord.csv',\n",
        "         'drive/My Drive/Colab/TFM/aristo-ord.csv',\n",
        "         'drive/My Drive/Colab/TFM/conceptnet-ord.csv'\n",
        "         ]\n",
        "\n",
        "for f in files:\n",
        "    df=pd.read_csv(f)\n",
        "    df['Offset1']=df['Word1'].apply(get_synset_offset) #for each offset, add the corresponding word in the row\n",
        "    df['Offset2']=df['Word2'].apply(get_synset_offset)\n",
        "    nuevo=os.path.splitext(f)[0] + \"-full.csv\"\n",
        "    df.to_csv(nuevo, index=False)"
      ],
      "metadata": {
        "id": "T6QIxJcKmRnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.2. Get word from an offset**"
      ],
      "metadata": {
        "id": "xj9oZZOr8Xf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_from_offset(offset):\n",
        "    pos=offset[-1]\n",
        "    synset_id=int(offset[:-2])\n",
        "\n",
        "    synset=wn.synset_from_pos_and_offset(pos, synset_id)\n",
        "    if synset:\n",
        "        return synset.lemma_names()[0]\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "-YJnz9Qk8bzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"drive/My Drive/Colab/TFM/visualg-ord.csv\"\n",
        "df=pd.read_csv(input)\n",
        "df['Word1']=df['Offset1'].apply(get_word_from_offset) #add word to each offset\n",
        "df['Word2']=df['Offset2'].apply(get_word_from_offset)\n",
        "#print(df)\n",
        "\n",
        "df.to_csv(\"drive/My Drive/Colab/TFM/visualg-ord-full.csv\", index = False)"
      ],
      "metadata": {
        "id": "PRIO7YiqmV0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. CLEANING**"
      ],
      "metadata": {
        "id": "gHsny51tE07u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_file(file):\n",
        "  df = pd.read_csv(file)\n",
        "  df = df.dropna(subset=['Offset1', 'Offset2'])\n",
        "  df = df.dropna(subset=['Word1', 'Word2'])\n",
        "  df = df[~(df['Word1'].str.contains(' ') | df['Word2'].str.contains(' '))] #multiple words\n",
        "  df.to_csv(file, index=False)\n",
        "  print(len(df))\n",
        "files = ['drive/My Drive/Colab/TFM/transomcs-ord-full.csv',\n",
        "        'drive/My Drive/Colab/TFM/ascent-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/quasimodo-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/uncommon-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/aristo-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/conceptnet-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/visualg-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/haspart-ord.csv'\n",
        "         ]\n",
        "for file in files:\n",
        "    file=clean_file(file)\n",
        "\n",
        "print(f\"ok\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_utBUs1ME1fo",
        "outputId": "685a0d86-3087-40b7-9090-32d998b104b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1616507\n",
            "40751\n",
            "27096\n",
            "19889\n",
            "27744\n",
            "3369\n",
            "31363\n",
            "34893\n",
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save files in one document\n",
        "def combine(files, outputfile):\n",
        "    dfs = []\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "        dfs.append(df)\n",
        "\n",
        "    comb=pd.concat(dfs, ignore_index=True) #join df\n",
        "    print(len(comb))\n",
        "    print(comb.head(5))\n",
        "    print(comb.tail(5))\n",
        "    comb.to_csv(outputfile, sep=',', index=False)\n",
        "\n",
        "files = ['drive/My Drive/Colab/TFM/transomcs-ord-full.csv',\n",
        "        'drive/My Drive/Colab/TFM/ascent-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/quasimodo-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/uncommon-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/aristo-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/conceptnet-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/visualg-ord-full.csv',\n",
        "         'drive/My Drive/Colab/TFM/haspart-ord.csv'\n",
        "         ]\n",
        "outputfile = \"drive/My Drive/Colab/TFM/CSKB.csv\"\n",
        "combine(files, outputfile)\n",
        "\n",
        "print(f\"ok\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL1GmUbVk3A7",
        "outputId": "ce5df6bb-8ba6-4c64-d4e8-bdf5a9110e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1801612\n",
            "     Word1   Word2     Offset1     Offset2 Resource\n",
            "0    right  people  05174653-n  07942152-n        T\n",
            "1  feeling  animal  00026192-n  00015388-n        T\n",
            "2  bedroom   house  02821627-n  03544360-n        T\n",
            "3  meaning    word  06601327-n  06286395-n        T\n",
            "4  feeling  people  00026192-n  07942152-n        T\n",
            "                   Word1         Word2     Offset1     Offset2 Resource\n",
            "1801607           animal        zygote  00015388-n  05431926-n       HP\n",
            "1801608              egg        zygote  01460457-n  05431926-n       HP\n",
            "1801609    female_'s_egg        zygote  01460457-n  05431926-n       HP\n",
            "1801610           specie        zygote  08110373-n  05431926-n       HP\n",
            "1801611  large_intestine  fermentation  05535484-n  13575433-n       HP\n",
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4. APPEARANCES**\n",
        "\n"
      ],
      "metadata": {
        "id": "g1yhoYYPXdgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.1. Times that a relationship appears**\n"
      ],
      "metadata": {
        "id": "7laZSHIP9uHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def appearances(input_file):\n",
        "    df = pd.read_csv(input_file)\n",
        "    grouped = df.groupby(['Word1', 'Word2', 'Offset1', 'Offset2']).agg({'Resource': lambda x: ', '.join(sorted(x.unique()))}).reset_index()\n",
        "    grouped['Appearances'] = df.groupby(['Word1', 'Word2', 'Offset1', 'Offset2']).size().values\n",
        "    output_file = \"drive/My Drive/Colab/TFM/appearances-2.csv\"\n",
        "    grouped.to_csv(output_file, index=False)\n",
        "    print(grouped)\n",
        "\n",
        "input_file = \"drive/My Drive/Colab/TFM/CSKB.csv\"\n",
        "appearances(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecViRKdtXbr0",
        "outputId": "130acac9-a311-4fc8-e31c-7795c671eff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             Word1      Word2     Offset1     Offset2  \\\n",
            "0        -PRON-_be_skeletal_muscle   creatine  05289861-n  14825243-n   \n",
            "1                               10       arms  13746512-n  04566257-n   \n",
            "2                             1000       feet  13750844-n  05563266-n   \n",
            "3                            10000       feet  13751265-n  05563266-n   \n",
            "4                               11       nose  13746672-n  05598147-n   \n",
            "...                            ...        ...         ...         ...   \n",
            "1732403                     zygote    vacuole  05431926-n  05447087-n   \n",
            "1732404                   zygotene   prophase  13575109-n  13542474-n   \n",
            "1732405                    zygotic       post  02882275-a  08624385-n   \n",
            "1732406                     zymase  phosphate  15109586-n  14982265-n   \n",
            "1732407                    zymurgy   magazine  06080361-n  06595351-n   \n",
            "\n",
            "        Resource  Appearances  \n",
            "0             HP            1  \n",
            "1              Q            1  \n",
            "2              Q            1  \n",
            "3              Q            1  \n",
            "4              Q            1  \n",
            "...          ...          ...  \n",
            "1732403       Ar            1  \n",
            "1732404        C            1  \n",
            "1732405        T            1  \n",
            "1732406       HP            1  \n",
            "1732407        T            1  \n",
            "\n",
            "[1732408 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.2. Groups**"
      ],
      "metadata": {
        "id": "40xp8csO959R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/My Drive/Colab/TFM/appearances-2.csv')\n",
        "\n",
        "count = df['Appearances'].value_counts().sort_index() #count groups\n",
        "for numero, cantidad in count.items():\n",
        "    print(f\"{numero}: {cantidad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHuyI2EL990y",
        "outputId": "57f1436b-da37-4b7c-8e83-766ff77c7291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 1671832\n",
            "2: 54009\n",
            "3: 5135\n",
            "4: 1014\n",
            "5: 272\n",
            "6: 101\n",
            "7: 33\n",
            "8: 9\n",
            "9: 2\n",
            "14: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.3. Random samples**\n",
        "150 random samples to evaluate manually."
      ],
      "metadata": {
        "id": "LRZ-mCfPApsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "df = pd.read_csv('drive/My Drive/Colab/TFM/appearances-2.csv')\n",
        "counts = df['Appearances'].value_counts()\n",
        "\n",
        "muestras=150\n",
        "selection={1:30, 2:20, 3:30, 4:20, 5:20, 6:17, 7:10, 8:3}\n",
        "muestrasrandom= []\n",
        "\n",
        "for numero, cantidad_muestras in selection.items():\n",
        "    subset = df[df['Appearances'] == numero] #only for this column\n",
        "    if len(subset) >= cantidad_muestras:\n",
        "        muestras = subset.sample(n=cantidad_muestras, random_state=1)\n",
        "        muestrasrandom.extend(muestras.values)"
      ],
      "metadata": {
        "id": "NNPXYHQMAyhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in muestrasrandom:\n",
        "    print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCIjVQeqmzK1",
        "outputId": "11727ae1-37eb-4f4e-dc0a-dce45a21e08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random samples:\n",
            "['martyr' 'wheelchair' '10296618-n' '04576002-n' 'T' 1]\n",
            "['win' 'hound' '07354731-n' '02087551-n' 'T' 1]\n",
            "['touch' 'bill' '07409592-n' '06536853-n' 'T' 1]\n",
            "['check' 'type' '13381734-n' '05840188-n' 'T' 1]\n",
            "['dip' 'baby' '13904506-n' '09827683-n' 'T' 1]\n",
            "['actor' 'doubles' '09765278-n' '00483605-n' 'Q' 1]\n",
            "['aversion' 'alcohol' '07502669-n' '07884567-n' 'T' 1]\n",
            "['call' 'driver' '06272803-n' '10034906-n' 'T' 1]\n",
            "['balance' 'plan' '14002279-n' '05898568-n' 'T' 1]\n",
            "['party' 'draft' '08256968-n' '13377268-n' 'T' 1]\n",
            "['phaeton' 'i' '04459122-n' '14641397-n' 'T' 1]\n",
            "['dessert' 'space' '07609840-n' '00028651-n' 'T' 1]\n",
            "['move' 'bird' '00165942-n' '01503061-n' 'T' 1]\n",
            "['agoraphobia' 'woman' '14381840-n' '10787470-n' 'T' 1]\n",
            "['place' 'osmosis' '08664443-n' '13528100-n' 'T' 1]\n",
            "['world' 'galaxy' '09466280-n' '08270938-n' 'T' 1]\n",
            "['signpost' 'banner' '06794374-n' '02788021-n' 'VG' 1]\n",
            "['occupation' 'armchair' '00582388-n' '02738535-n' 'T' 1]\n",
            "['code' 'zone' '06667317-n' '08688247-n' 'T' 1]\n",
            "['omaha' 'style' '09663628-n' '04928903-n' 'T' 1]\n",
            "['ulcer' 'victim' '14211609-n' '10752093-n' 'T' 1]\n",
            "['grade' 'food' '08238463-n' '00021265-n' 'T' 1]\n",
            "['body' 'mine' '05216365-n' '03768346-n' 'T' 1]\n",
            "['goodness' 'reader' '05142180-n' '10508710-n' 'T' 1]\n",
            "['spare' 'owner' '04267870-n' '10388924-n' 'T' 1]\n",
            "['serviceman' 'arm' '10582746-n' '05563770-n' 'Ar' 1]\n",
            "['form' 'roof' '06290637-n' '04105068-n' 'T' 1]\n",
            "['father' 'hemochromatosis' '10080869-n' '14206929-n' 'T' 1]\n",
            "['tax' 'i' '13308999-n' '14641397-n' 'T' 1]\n",
            "['honeybee' 'summer' '02208280-n' '15237250-n' 'T' 1]\n",
            "['team' 'division' '08208560-n' '08213205-n' 'T' 2]\n",
            "['scooter' 'friend' '04562122-n' '10112591-n' 'T' 2]\n",
            "['coach' 'opportunity' '09931640-n' '14483917-n' 'T' 2]\n",
            "['database' 'database' '06637824-n' '06637824-n' 'T' 2]\n",
            "['manager' 'say' '10014939-n' '14485526-n' 'T' 2]\n",
            "['company' 'truck' '08058098-n' '04490091-n' 'T' 2]\n",
            "['religion' 'follower' '05946687-n' '10099375-n' 'A++, T' 2]\n",
            "['pressure' 'tire' '11495041-n' '04440749-n' 'T' 2]\n",
            "['America' 'Louisiana' '09044862-n' '09090825-n' 'Ar' 2]\n",
            "['edit' 'edit' '00200397-v' '00200397-v' 'T' 2]\n",
            "['boy' 'leg' '10285313-n' '05560787-n' 'Ar, T' 2]\n",
            "['letter' 'sound' '06624161-n' '04981139-n' 'T' 2]\n",
            "['love' 'wing' '07543288-n' '02151625-n' 'T' 2]\n",
            "['he' 'past' '14639921-n' '15120823-n' 'T' 2]\n",
            "['place' 'concept' '08664443-n' '05835747-n' 'T' 2]\n",
            "['flower' 'placenta' '11669921-n' '11680838-n' 'Ar' 2]\n",
            "['basket' 'hat' '02801938-n' '03497657-n' 'A++, T' 2]\n",
            "['expression' 'process' '04679738-n' '01023820-n' 'T, Un' 2]\n",
            "['candidate' 'drug' '09889691-n' '03247620-n' 'T' 2]\n",
            "['gun' 'business' '03467984-n' '08061042-n' 'T' 2]\n",
            "['finger' 'pad' '05566504-n' '14973133-n' 'Ar, T' 3]\n",
            "['door' 'steel' '03221720-n' '14802450-n' 'A++, T' 3]\n",
            "['bird' 'toe' '01503061-n' '05577410-n' 'HP, T, VG' 3]\n",
            "['lawyer' 'lawyer' '10249950-n' '10249950-n' 'A++, T' 3]\n",
            "['penguin' 'belly' '02055803-n' '05556943-n' 'Ar, HP, Q' 3]\n",
            "['train' 'guard' '04468005-n' '10150071-n' 'A++, T, VG' 3]\n",
            "['termite' 'wing' '02223266-n' '02151625-n' 'A++, HP' 3]\n",
            "['bed' 'corner' '02818832-n' '08544419-n' 'T, VG' 3]\n",
            "['ant' 'brain' '02219486-n' '05481095-n' 'Ar, HP, T' 3]\n",
            "['egg' 'chromatin' '01460457-n' '05436080-n' 'Ar, HP' 3]\n",
            "['nucleus' 'gene' '05434927-n' '05436752-n' 'Ar, HP' 3]\n",
            "['agency' 'resource' '08337324-n' '13331778-n' 'A++, T' 3]\n",
            "['machine' 'screen' '03699975-n' '04152829-n' 'T, VG' 3]\n",
            "['mask' 'eye' '03724870-n' '05311054-n' 'HP, T, VG' 3]\n",
            "['driver' 'foot' '10034906-n' '05563266-n' 'T, VG' 3]\n",
            "['house' 'base' '03544360-n' '02798290-n' 'Ar, T, VG' 3]\n",
            "['man' 'body' '10287213-n' '05216365-n' 'T, VG' 3]\n",
            "['hydra' 'head' '09496261-n' '05538625-n' 'Ar, T' 3]\n",
            "['bottle' 'neck' '02876657-n' '05546540-n' 'A++, Q, VG' 3]\n",
            "['page' 'form' '06256697-n' '06290637-n' 'A++, T' 3]\n",
            "['boy' 'game' '10285313-n' '00455599-n' 'A++, T' 3]\n",
            "['change' 'impact' '07296428-n' '07338552-n' 'A++, T' 3]\n",
            "['city' 'policy' '08524735-n' '05901508-n' 'A++, T' 3]\n",
            "['family' 'chicken' '08078020-n' '07644967-n' 'A++, T' 3]\n",
            "['process' 'effect' '01023820-n' '11410625-n' 'A++, T' 3]\n",
            "['bed' 'post' '02818832-n' '08624385-n' 'A++, T, VG' 3]\n",
            "['wash' 'face' '04553389-n' '05600637-n' 'Q, T' 3]\n",
            "['dog' 'tag' '02084071-n' '07273136-n' 'A++, T, VG' 3]\n",
            "['dragonfly' 'eye' '02268443-n' '05311054-n' 'A++, Ar, HP' 3]\n",
            "['machine' 'wheel' '03699975-n' '04574999-n' 'A++, T, VG' 3]\n",
            "['toothbrush' 'bristle' '04453156-n' '02902535-n' 'Ar, HP, VG' 4]\n",
            "['man' 'thigh' '10287213-n' '05562249-n' 'HP, T, VG' 4]\n",
            "['doctor' 'concern' '10020890-n' '05670710-n' 'A++, T' 4]\n",
            "['horse' 'mane' '02374451-n' '01899746-n' 'A++, Q, T, VG' 4]\n",
            "['wine' 'sugar' '07891726-n' '07859284-n' 'A++, HP, Q, T' 4]\n",
            "['country' 'tradition' '08168978-n' '05809745-n' 'A++, T' 4]\n",
            "['rat' 'ear' '02331046-n' '05320899-n' 'Ar, HP, T, VG' 4]\n",
            "['woman' 'limit' '10787470-n' '05124057-n' 'A++, T' 4]\n",
            "['eagle' 'talon' '01613294-n' '02156413-n' 'Ar, HP, VG' 4]\n",
            "['barn' 'roof' '02793495-n' '04105068-n' 'Ar, T, VG' 4]\n",
            "['god' 'kingdom' '09536058-n' '14514805-n' 'A++, Q, T' 4]\n",
            "['butterfly' 'wing' '02274259-n' '02151625-n' 'Ar, HP, T, VG' 4]\n",
            "['film' 'music' '06613686-n' '07020895-n' 'A++, Q, T' 4]\n",
            "['couple' 'baby' '07985628-n' '09827683-n' 'A++, Q, T' 4]\n",
            "['husband' 'daughter' '10193967-n' '09992837-n' 'A++, T' 4]\n",
            "['building' 'iron' '02913152-n' '14642417-n' 'A++, T' 4]\n",
            "['bottle' 'cap' '02876657-n' '02954340-n' 'A++, T, VG' 4]\n",
            "['alloy' 'iron' '14586769-n' '14642417-n' 'A++, HP, T' 4]\n",
            "['tail' 'tip' '02157557-n' '08663156-n' 'HP, T, VG' 4]\n",
            "['horse' 'stomach' '02374451-n' '05395690-n' 'Ar, HP, Q, VG' 4]\n",
            "['phone' 'feature' '04401088-n' '05849789-n' 'A++, T' 5]\n",
            "['committee' 'jurisdiction' '08324514-n' '05198427-n' 'A++, T' 5]\n",
            "['school' 'library' '08276720-n' '03660909-n' 'A++, Q, T' 5]\n",
            "['eagle' 'wing' '01613294-n' '02151625-n' 'A++, Ar, HP, T, VG' 5]\n",
            "['atom' 'neutron' '14619225-n' '09369520-n' 'A++, HP, T' 5]\n",
            "['bear' 'hair' '02131653-n' '05254795-n' 'HP, Q, T, VG' 5]\n",
            "['desert' 'sand' '08505573-n' '15019030-n' 'HP, Q, T, VG' 5]\n",
            "['guy' 'arm' '10153414-n' '05563770-n' 'A++, Ar, T, VG' 5]\n",
            "['parent' 'friend' '10399491-n' '10112591-n' 'A++, T' 5]\n",
            "['human' 'hand' '02472293-n' '05564590-n' 'A++, Ar, Q, T' 5]\n",
            "['woman' 'body' '10787470-n' '05216365-n' 'HP, Q, T, VG' 5]\n",
            "['bull' 'horn' '02403325-n' '03537085-n' 'A++, Ar, T, VG' 5]\n",
            "['shirt' 'stripe' '04197391-n' '02784732-n' 'A++, Ar, T' 5]\n",
            "['duck' 'bill' '01846331-n' '06536853-n' 'A++, Q, T, VG' 5]\n",
            "['baby' 'blood' '09827683-n' '05399847-n' 'A++, HP, Q, T' 5]\n",
            "['building' 'glass' '02913152-n' '14881303-n' 'A++, T, VG' 5]\n",
            "['animal' 'tail' '00015388-n' '02157557-n' 'Ar, HP, Q, T, VG' 5]\n",
            "['god' 'compassion' '09536058-n' '07553741-n' 'A++, T' 5]\n",
            "['driver' 'license' '10034906-n' '06549661-n' 'A++, Q, T' 5]\n",
            "['fish' 'mouth' '02512053-n' '05302499-n' 'A++, Ar, HP, T, VG' 5]\n",
            "['woman' 'career' '10787470-n' '00583246-n' 'A++, T' 6]\n",
            "['planet' 'gas' '09394007-n' '14481080-n' 'A++, HP, Q, T' 6]\n",
            "['child' 'desire' '09917593-n' '07484265-n' 'A++, T' 6]\n",
            "['woman' 'voice' '10787470-n' '04981941-n' 'A++, Q, T' 6]\n",
            "['dragon' 'wing' '09494388-n' '02151625-n' 'A++, HP, T, VG' 6]\n",
            "['shirt' 'cuff' '04197391-n' '03145843-n' 'A++, Ar, T, VG' 6]\n",
            "['hotel' 'window' '03542333-n' '04587648-n' 'A++, Ar, T, VG' 6]\n",
            "['shark' 'bone' '01482330-n' '05269901-n' 'A++, Ar, HP, Q, T' 6]\n",
            "['sandwich' 'bread' '07695965-n' '07679356-n' 'A++, Ar, T, VG' 6]\n",
            "['hawk' 'eye' '01605630-n' '05311054-n' 'Ar, HP, Q, T, VG' 6]\n",
            "['woman' 'control' '10787470-n' '05196375-n' 'A++, T, VG' 6]\n",
            "['food' 'preservative' '00021265-n' '14997529-n' 'A++, HP' 6]\n",
            "['dog' 'fur' '02084071-n' '14764061-n' 'A++, HP, Q, T, VG' 6]\n",
            "['universe' 'beginning' '09466280-n' '07290905-n' 'A++, Q, T' 6]\n",
            "['menu' 'item' '06492939-n' '06482401-n' 'A++, Ar, T, VG' 6]\n",
            "['dog' 'skin' '02084071-n' '05238282-n' 'Ar, Q, T, VG' 6]\n",
            "['mother' 'baby' '10332385-n' '09827683-n' 'A++, T, VG' 6]\n",
            "['food' 'sugar' '00021265-n' '07859284-n' 'A++, HP, T, VG' 7]\n",
            "['hand' 'finger' '05564590-n' '05566504-n' 'A++, Ar, HP, T, VG' 7]\n",
            "['baby' 'head' '09827683-n' '05538625-n' 'Ar, HP, Q, T, VG' 7]\n",
            "['mouse' 'tail' '02330245-n' '02157557-n' 'A++, Ar, HP, Q, T, VG' 7]\n",
            "['tree' 'trunk' '13104059-n' '13165815-n' 'A++, Ar, HP, T, VG' 7]\n",
            "['mother' 'child' '10332385-n' '09917593-n' 'A++, Q, T, VG' 7]\n",
            "['hand' 'palm' '05564590-n' '05565696-n' 'A++, Ar, HP, T, VG' 7]\n",
            "['monkey' 'tail' '02484322-n' '02157557-n' 'A++, Ar, HP, Q, T, VG' 7]\n",
            "['immigrant' 'right' '10199489-n' '05174653-n' 'A++, T' 7]\n",
            "['student' 'right' '10665698-n' '05174653-n' 'A++, T' 7]\n",
            "['tree' 'bark' '13104059-n' '13162297-n' 'A++, Ar, HP, Q, T, VG' 8]\n",
            "['cat' 'tail' '02121620-n' '02157557-n' 'A++, Ar, HP, Q, T, VG' 8]\n",
            "['lion' 'mane' '02129165-n' '01899746-n' 'A++, Ar, HP, Q, T, VG' 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reorder and save\n",
        "df_final=pd.DataFrame(muestrasrandom, columns=df.columns)\n",
        "df_final_ord=df_final.sort_values(by='Appearances', ascending=False)\n",
        "df_final_ord.to_csv('drive/My Drive/Colab/TFM/PRUEBA/relations-2.csv', sep='\\t', index=False)\n",
        "print(len(df_final_ord))"
      ],
      "metadata": {
        "id": "POFB3JiuASi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbfc35b-b894-4595-bf48-48f9b5a5a16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n"
          ]
        }
      ]
    }
  ]
}